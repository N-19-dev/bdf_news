# Guide des APIs LLM pour Veille Tech

## üéØ R√©sum√© Rapide

**GROQ EST D√âJ√Ä GRATUIT - PAS BESOIN DE CHANGER !** ‚úÖ

Les rate limits que vous voyez ne viennent PAS du LLM, mais des sources RSS (Hashnode).

---

## üÜì Option 1 : Groq (ACTUEL - RECOMMAND√â)

### Configuration actuelle (config.yaml)
```yaml
llm:
  provider: "openai_compat"
  base_url: "https://api.groq.com/openai/v1"
  api_key_env: "GROQ_API_KEY"
  model: "llama-3.1-8b-instant"
  temperature: 0.2
  max_tokens: 1200
```

### Obtenir une cl√© Groq GRATUITE

1. **Cr√©er un compte :** https://console.groq.com
2. **G√©n√©rer une cl√© API :** Settings ‚Üí API Keys ‚Üí Create API Key
3. **Configurer dans .env :**
   ```bash
   cd backend
   echo "GROQ_API_KEY=gsk_votre_cl√©_ici" >> .env
   ```

### Quotas GRATUITS Groq
- **llama-3.1-8b-instant :** 30 requ√™tes/minute
- **Limite journali√®re :** ~14,400 requ√™tes/jour
- **Co√ªt :** $0 (100% gratuit)
- **Vitesse :** Tr√®s rapide (hardware sp√©cialis√© LPU)

### Utilisation estim√©e pour ce projet
- Crawl hebdomadaire : ~100 articles
- Classification LLM : ~100 requ√™tes
- R√©sum√© : 10 requ√™tes
- **Total : ~110 requ√™tes/semaine**
- **Verdict : LARGEMENT DANS LE QUOTA GRATUIT** ‚úÖ

---

## üí∞ Option 2 : Mistral AI (PAYANT)

### Configuration Mistral
```yaml
llm:
  provider: "openai_compat"
  base_url: "https://api.mistral.ai/v1"
  api_key_env: "MISTRAL_API_KEY"
  model: "mistral-small-latest"  # ou "open-mistral-7b"
  temperature: 0.2
  max_tokens: 1200
```

### Tarifs Mistral (d√©cembre 2024)
- **open-mistral-7b :** $0.10 / 1M tokens input, $0.10 / 1M tokens output
- **mistral-small :** $0.10 / 1M tokens input, $0.30 / 1M tokens output
- **mistral-medium :** $0.70 / 1M tokens input, $2.10 / 1M tokens output

### Co√ªt estim√© mensuel (4 crawls/mois)
- Tokens utilis√©s : ~400k tokens/mois (100k/crawl)
- Avec mistral-small : ~$0.16/mois
- **Verdict : Tr√®s peu cher, mais Groq est GRATUIT** üí°

### Obtenir une cl√© Mistral
1. https://console.mistral.ai
2. Cr√©er un compte (carte bancaire requise)
3. G√©n√©rer API Key
4. Ajouter √† .env : `MISTRAL_API_KEY=...`

---

## üöÄ Option 3 : OpenAI (PAYANT, CHER)

### Configuration OpenAI
```yaml
llm:
  provider: "openai_compat"
  base_url: "https://api.openai.com/v1"
  api_key_env: "OPENAI_API_KEY"
  model: "gpt-4o-mini"  # ou "gpt-3.5-turbo"
  temperature: 0.2
  max_tokens: 1200
```

### Tarifs OpenAI (d√©cembre 2024)
- **gpt-4o-mini :** $0.15 / 1M tokens input, $0.60 / 1M tokens output
- **gpt-3.5-turbo :** $0.50 / 1M tokens input, $1.50 / 1M tokens output
- **gpt-4 :** $30 / 1M tokens input, $60 / 1M tokens output

### Co√ªt estim√© mensuel
- Avec gpt-4o-mini : ~$0.30/mois
- Avec gpt-3.5-turbo : ~$0.80/mois
- **Verdict : Plus cher que Mistral, Groq est GRATUIT** üí∏

---

## üÜì Option 4 : Ollama (LOCAL - GRATUIT)

### Avantages
- ‚úÖ 100% gratuit
- ‚úÖ Pas de limites de requ√™tes
- ‚úÖ Donn√©es priv√©es (tout local)

### Inconv√©nients
- ‚ùå N√©cessite une bonne machine (16GB RAM minimum)
- ‚ùå Plus lent que les APIs cloud
- ‚ùå N√©cessite installation et configuration

### Installation Ollama
```bash
# MacOS/Linux
curl -fsSL https://ollama.com/install.sh | sh

# T√©l√©charger un mod√®le
ollama pull llama3.1:8b

# Lancer le serveur
ollama serve
```

### Configuration dans config.yaml
```yaml
llm:
  provider: "openai_compat"
  base_url: "http://localhost:11434/v1"
  api_key_env: "OLLAMA_API_KEY"  # Peut √™tre vide
  model: "llama3.1:8b"
  temperature: 0.2
  max_tokens: 1200
```

---

## üîß R√âSOUDRE LES RATE LIMITS (CRAWLING RSS)

### Probl√®me identifi√©
Les rate limits viennent de **Hashnode** (sources RSS), PAS du LLM !

### Solution 1 : Supprimer Hashnode (FAIT ‚úÖ)
```yaml
# Dans config.yaml - D√âJ√Ä FAIT
# - name: Hashnode ¬∑ data
#   url: https://hashnode.com/n/data/rss
# - name: Hashnode ¬∑ machine-learning
#   url: https://hashnode.com/n/machine-learning/rss
```

### Solution 2 : R√©duire le rate limiting (FAIT ‚úÖ)
```yaml
crawl:
  concurrency: 8      # R√©duit de 12 √† 8
  per_host_rps: 1.0   # R√©duit de 1.5 √† 1.0
```

### Solution 3 : Ajouter des d√©lais entre sources
```yaml
crawl:
  concurrency: 4      # Encore plus conservateur
  per_host_rps: 0.5   # 1 requ√™te toutes les 2 secondes
```

---

## üìä COMPARATIF FINAL

| Fournisseur | Co√ªt/mois | Vitesse | Quota | Recommandation |
|-------------|-----------|---------|-------|----------------|
| **Groq** ‚úÖ | $0 | ‚ö°‚ö°‚ö° Tr√®s rapide | 14k req/jour | **MEILLEUR CHOIX** |
| Mistral | ~$0.16 | ‚ö°‚ö° Rapide | Payant | Si quota Groq d√©pass√© |
| OpenAI | ~$0.30+ | ‚ö°‚ö° Rapide | Payant | Plus cher |
| Ollama | $0 | ‚ö° Lent | Illimit√© | Si machine puissante |

---

## üéØ RECOMMANDATION FINALE

### Pour ce projet : **GROQ (ACTUEL)** ‚úÖ

**Pourquoi ?**
1. ‚úÖ 100% gratuit
2. ‚úÖ Tr√®s rapide (LPU hardware)
3. ‚úÖ Quota largement suffisant (14k req/jour vs ~110 req/semaine)
4. ‚úÖ D√©j√† configur√© dans le code
5. ‚úÖ Pas de carte bancaire n√©cessaire

### O√π payer SI N√âCESSAIRE ?

**Sc√©nario 1 : Quota Groq d√©pass√©**
- Passer √† Mistral (~$0.20/mois)
- Ou OpenAI gpt-4o-mini (~$0.30/mois)

**Sc√©nario 2 : Meilleure qualit√© LLM**
- OpenAI gpt-4 (~$12/mois pour ce projet)
- Claude 3.5 Sonnet via Anthropic API (~$10/mois)

**Sc√©nario 3 : Zero co√ªt**
- Installer Ollama en local (gratuit, mais n√©cessite bonne machine)

---

## üöÄ ACTIONS √Ä FAIRE MAINTENANT

1. **Obtenir une cl√© Groq (GRATUIT) :**
   ```bash
   # 1. Aller sur https://console.groq.com
   # 2. Cr√©er un compte
   # 3. G√©n√©rer une cl√© API
   # 4. Ajouter √† .env
   cd backend
   echo "GROQ_API_KEY=gsk_votre_cl√©" >> .env
   ```

2. **Relancer le pipeline :**
   ```bash
   source .venv/bin/activate
   python regenerate_weeks.py
   ```

3. **V√©rifier les r√©sultats :**
   - Plus de REX d√©tect√©s (classification LLM active)
   - R√©sum√©s hebdomadaires g√©n√©r√©s
   - Plus d'erreurs de rate limit (Hashnode supprim√©)

---

## üìù EN CAS DE PROBL√àME

### "GROQ_API_KEY manquant"
```bash
# V√©rifier que .env existe
cat backend/.env

# Ajouter la cl√© si manquante
echo "GROQ_API_KEY=gsk_votre_cl√©" >> backend/.env
```

### "Rate limit exceeded" (Groq)
- Passer au mod√®le gratuit le plus lent : `llama-3.1-70b-versatile`
- Ou passer √† Mistral (~$0.20/mois)

### "Trop lent"
- Garder Groq (d√©j√† le plus rapide)
- Ou augmenter `concurrency` dans config.yaml

### "Qualit√© LLM insuffisante"
- Passer √† OpenAI gpt-4o-mini (~$0.30/mois)
- Ou Claude 3.5 Sonnet (~$10/mois)
