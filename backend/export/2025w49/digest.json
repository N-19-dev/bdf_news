{
  "ai_data_engineering": [
    {
      "url": "https://blog.octo.com/sopo-2025-from-0-to-ai--product-marketing-lessons-from-launching-create-with-ai-dominique-rolink",
      "title": "[SoPo 2025] From 0 to AI : Product Marketing lessons from launching Create with AI - Dominique Rolink",
      "summary": "Comment positionner l’IA comme un outil accessible plutôt qu’un saut dans l’inconnu ?Dominique Rolink PMM chez Miro nous donne quelques clés pour encourager l'adoption d'un produit IA.",
      "published_ts": 1765135390,
      "source_name": "OCTO Talks!",
      "content_type": "technical"
    },
    {
      "url": "https://www.dataiku.com/stories/blog/maximizing-enterprise-data-products-distribution",
      "title": "Maximizing Enterprise Data Products Distribution",
      "summary": "What’s the difference between traditional data outputs and data products? Jean-Guillaume Appert , senior director of product management at Dataiku, and Marko Stojsavljevic , business transformation expert at Dataiku, answered this question (and more!) in a recent Dataiku Product Days session.",
      "published_ts": 1764964171,
      "source_name": "Dataiku Blog",
      "content_type": "technical"
    },
    {
      "url": "https://www.dataiku.com/stories/blog/ai-augmenting-decision-making",
      "title": "AI Isn't Taking Over, It's Augmenting Decision-Making",
      "summary": "It's a Saturday afternoon, and I'm at a kid's birthday party. The obligatory small talk is in full swing, and after discussing whose kid belongs to whom and comparing notes on cupcake flavors, I hear the inevitable question, \"So, what do you do?\"",
      "published_ts": 1764952290,
      "source_name": "Dataiku Blog",
      "content_type": "technical"
    },
    {
      "url": "https://blog.octo.com/school-of-product-2025-compte-rendu-du-talk-accelerez-la-croissance-de-vos-produits-par-le-content-design",
      "title": "School of Product 2025-Compte rendu du talk: Accélérez la croissance de vos produits par le Content Design",
      "summary": "Eloïse Marcé, Product & Content Designer chez OCTO Technology, experte Product & Content, défend une conviction forte : les mots sont un véritable outil de croissance.\nElle ouvre sa conférence par une question marquante :« Et si l’on pouvait réduire un drop de 40 %… simplement en choisissant mieux s",
      "published_ts": 1764950290,
      "source_name": "OCTO Talks!",
      "content_type": "technical"
    },
    {
      "url": "https://blog.octo.com/cr-school-of-product-2025-table-ronde-et-si-ralentir-faisait-gagner-du-temps-",
      "title": "CR School of Product 2025 - Table ronde \"Et si ralentir faisait gagner du temps ?\"",
      "summary": "CR de la table ronde  and quot;Et si ralentir faisait gagner du temps ? and quot; à la Conf' School of Product 2025.",
      "published_ts": 1764949870,
      "source_name": "OCTO Talks!",
      "content_type": "technical"
    },
    {
      "url": "https://www.dataiku.com/stories/blog/from-ai-wild-west-to-main-street",
      "title": "From AI Wild West to Main Street: Sustainability for Enterprise Agents",
      "summary": "Over the last two years, AI agents have exploded across the enterprise landscape. Marketing teams prototype assistants for campaign analytics . Operations teams build copilots for scheduling and logistics. Finance experiments with reconciliation agents. Product teams embed autonomous workflows into customer-facing features.",
      "published_ts": 1764943380,
      "source_name": "Dataiku Blog",
      "content_type": "technical"
    },
    {
      "url": "https://www.dataiku.com/stories/blog/ai-literacy-chro",
      "title": "AI Literacy: CHRO’s Strategic Lever for Talent Transformation",
      "summary": "AI (including Generative AI) is reshaping the workplace, and the speed of these changes demands that companies equip their workforce to navigate and leverage this new technology. For organizations to stay competitive and agile, employees need to be more than just users of AI — they need to understand how it works and how it can amplify their roles.",
      "published_ts": 1764900393,
      "source_name": "Dataiku Blog",
      "content_type": "technical"
    },
    {
      "url": "https://www.databricks.com/blog/building-ai-ready-enterprise-leaders-share-real-world-ai-solutions-and-practices",
      "title": "Building the AI-Ready Enterprise: Leaders Share Real-World AI Solutions and Practices",
      "summary": "AI adoption is progressing at a faster pace than any previous technology cycle, and...",
      "published_ts": 1764878400,
      "source_name": "Databricks Blog",
      "content_type": "technical"
    },
    {
      "url": "https://blog.octo.com/school-of-product-2025-compte-rendu-du-talk--de-la-contrainte-a-la-creativite--le-rex-du-lancement-des-shorts-chez-arte",
      "title": "School of Product 2025 - Compte-rendu du talk : De la contrainte à la créativité : le REX du lancement des shorts chez ARTE",
      "summary": "Olivier Hoffschir, Product Manager chez Arte.tv, raconte comment un format vidéo vertical imposé par un financeur a donné naissance aux “shorts” sur l’app mobile. En quatre mois, l’équipe a conçu un MVP simple mais engageant, test en production inclus, doublant la rétention mobile et révélant le chaos derrière la simplicité",
      "published_ts": 1764859270,
      "source_name": "OCTO Talks!",
      "content_type": "technical"
    },
    {
      "url": "https://www.dataiku.com/stories/blog/european-air-transport",
      "title": "How European Air Transport Built AI Agents for 80x Operational Efficiency",
      "summary": "With AI agents in Dataiku, European Air Transport automated document processing to achieve near real-time reporting that informs planning and critical decision-making. 80x more productive; 40–50 hours of manual work reduced to 30 minutes 1000s of documents processed automatically Reduced data latency for daily reports from days to just minutes",
      "published_ts": 1764853202,
      "source_name": "Dataiku Blog",
      "content_type": "technical"
    },
    {
      "url": "https://huggingface.co/blog/hf-skills-training",
      "title": "We Got Claude to Fine-Tune an Open Source LLM",
      "summary": "We Got Claude to Fine-Tune an Open Source LLM\nWe gave Claude the ability to fine-tune language models using a new tool called\nHugging Face Skills\n. Not just write training scripts, but to actually submit jobs to cloud GPUs, monitor progress, and push finished models to the Hugging Face Hub. This tut",
      "published_ts": 1764806400,
      "source_name": "Hugging Face Blog",
      "content_type": "technical"
    },
    {
      "url": "https://blog.langchain.com/evaluating-deep-agents-our-learnings/",
      "title": "Evaluating Deep Agents: Our Learnings",
      "summary": "Over the past month at LangChain, we shipped four applications on top of the Deep Agents harness: DeepAgents CLI : a coding agent LangSmith Assist: an in-app agent to help with various things in LangSmith Personal Email Assistant: an email assistant that learns from interactions with each user Agent Builder : a",
      "published_ts": 1764783857,
      "source_name": "LangChain Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/improve-model-accuracy-with-reinforcement-fine-tuning-in-amazon-bedrock/",
      "title": "Amazon Bedrock adds reinforcement ﬁne-tuning simplifying how developers build smarter, more accurate AI models",
      "summary": "Amazon Bedrock now supports reinforcement fine-tuning delivering 66% accuracy gains on average over base models.",
      "published_ts": 1764778094,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://www.dataiku.com/stories/blog/a-modern-approach-to-analytics-powered-by-dataiku-and-databricks",
      "title": "A Modern Approach to Analytics, Powered by Dataiku and Databricks",
      "summary": "Data and IT leaders are under increasing pressure to accelerate the delivery of AI results while  maintaining airtight governance . Data volumes are exploding, models are more complex, and business teams expect real-time, self-service capabilities. Traditional, siloed analytics systems built for batch processing and static reporting can no longer support the speed and scale of today’s enterprise.",
      "published_ts": 1764768023,
      "source_name": "Dataiku Blog",
      "content_type": "technical"
    },
    {
      "url": "https://engineering.salesforce.com/how-agentforce-achieved-3-5x-faster-response-times-while-solving-enterprise-scale-architectural-complexity/",
      "title": "How Agentforce Achieved 3–5x Faster Response Times While Solving Enterprise-Scale Architectural Complexity",
      "summary": "By Kunal Pal and Krista Hardebeck. In our Engineering Energizers Q&A series, we showcase the engineering minds driving innovation across Salesforce. Today, we introduce Krista Hardebeck, Regional Vice President of Forward Deployed Engineering (FDE), whose team collaborated with a large multi-brand specialty retailer to launch their initial production Agentforce service experience on an accelerated timeline. […] The post How Agentforce Achieved 3–5x Faster Response Times While Solving Enterprise-Scale Architectural Complexity appeared first on Salesforce Engineering Blog .",
      "published_ts": 1764735320,
      "source_name": "Salesforce Engineering",
      "content_type": "technical"
    },
    {
      "url": "https://huggingface.co/blog/nvidia/custom-policy-reasoning-nemotron-content-safety",
      "title": "Custom Policy Enforcement with Reasoning: Faster, Safer AI Applications",
      "summary": "Custom Policy Enforcement with Reasoning: Faster, Safer AI Applications\nMost safety models enforce a single, generalized policy that blocks obviously harmful content,  toxicity, and jailbreak attempts. That works for broad categories, but real-world applications demand more. Generic content safety m",
      "published_ts": 1764701428,
      "source_name": "Hugging Face Blog",
      "content_type": "technical"
    },
    {
      "url": "https://blog.langchain.com/langsmith-agent-builder-now-in-public-beta/",
      "title": "LangSmith Agent Builder now in Public Beta",
      "summary": "Now anyone can create production ready agents without writing code, just chat.\nAgent Builder guides you from initial idea to deployed agent, creating detailed prompts, selecting required tools, and even creating subagents.",
      "published_ts": 1764693039,
      "source_name": "LangChain Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/amazon-bedrock-agentcore-adds-quality-evaluations-and-policy-controls-for-deploying-trusted-ai-agents/",
      "title": "Amazon Bedrock AgentCore adds quality evaluations and policy controls for deploying trusted AI agents",
      "summary": "Deploy AI agents with confidence using new quality evaluations and policy controls—enabling precise boundaries on agent actions, continuous quality monitoring, and experience-based learning while maintaining natural conversation flows.",
      "published_ts": 1764692076,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/amazon-bedrock-adds-fully-managed-open-weight-models/",
      "title": "Amazon Bedrock adds 18 fully managed open weight models, including the new Mistral Large 3 and Ministral 3 models",
      "summary": "Access fully managed foundation models from leading providers like Google, Kimi AI, MiniMax AI, Mistral AI, NVIDIA, OpenAI, and Qwen, including the new Mistral Large 3 and Ministral 3 3B, 8B, and 14B models through Amazon Bedrock.",
      "published_ts": 1764691557,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/accelerate-ai-development-using-amazon-sagemaker-ai-with-serverless-mlflow/",
      "title": "Accelerate AI development using Amazon SageMaker AI with serverless MLflow",
      "summary": "Simplify AI experimentation with zero-infrastructure MLflow that launches in minutes, scales automatically, and seamlessly integrates with SageMaker's model customization and pipeline capabilities.",
      "published_ts": 1764691376,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/introducing-amazon-nova-2-lite-a-fast-cost-effective-reasoning-model/",
      "title": "Introducing Amazon Nova 2 Lite, a fast, cost-effective reasoning model",
      "summary": "New fast, cost-effective model supports extended thinking with adjustable reasoning depth, letting you control the balance between speed, intelligence, and cost while building AI applications for everyday workloads.",
      "published_ts": 1764691180,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/introducing-amazon-nova-forge-build-your-own-frontier-models-using-nova/",
      "title": "Introducing Amazon Nova Forge: Build your own frontier models using Nova",
      "summary": "New program gives organizations unprecedented access to Nova model training, enabling them to build custom frontier models that deeply embed domain expertise without the traditional barriers of cost, compute, and time.",
      "published_ts": 1764690831,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://blog.octo.com/la-school-of-product-2025-talk-d'olivier-girardot-construire-un-compagnon-ia-pour-les-hopitaux",
      "title": "La School of Product 2025 - Talk d'Olivier Girardot - Construire un compagnon IA pour les hôpitaux",
      "summary": "Dans ce contexte où l’IA est poussé partout et tout le temps, la conférence d’Olivier Girardot  rappelle que l’IA en santé doit rester utile, responsable et digne de confiance. Elle doit libérer du temps aux médecins tout en garantissant la souveraineté des données, malgré une maturité produit encore fragile et un marché en pleine effervescence.",
      "published_ts": 1764672430,
      "source_name": "OCTO Talks!",
      "content_type": "technical"
    },
    {
      "url": "https://www.databricks.com/blog/seamlessly-resume-sessions-serverless-notebooks",
      "title": "Seamlessly resume sessions in Serverless notebooks",
      "summary": "Serverless notebooks make working with data simple and efficient, but until now,...",
      "published_ts": 1764630653,
      "source_name": "Databricks Blog",
      "content_type": "technical"
    },
    {
      "url": "https://www.databricks.com/blog/databricks-and-nvidia-powering-next-generation-industry-ai",
      "title": "Databricks and NVIDIA: Powering the Next Generation of Industry AI",
      "summary": "Industry Use Case Transformation with AIAs we head to Las Vegas for Amazon Web Services (AWS) re:Invent...",
      "published_ts": 1764628200,
      "source_name": "Databricks Blog",
      "content_type": "technical"
    },
    {
      "url": "https://www.databricks.com/blog/genesis-workbench-blueprint-life-sciences-applications-databricks",
      "title": "Genesis Workbench: A Blueprint for Life Sciences Applications on Databricks",
      "summary": "AI is Accelerating Target Discovery and Transforming Drug DesignThe life sciences...",
      "published_ts": 1764610476,
      "source_name": "Databricks Blog",
      "content_type": "technical"
    },
    {
      "url": "https://slack.engineering/streamlining-security-investigations-with-agents/",
      "title": "Streamlining Security Investigations with Agents",
      "summary": "Slack’s Security Engineering team is responsible for protecting Slack’s core infrastructure and services. Our security event ingestion pipeline handles billions of events per day from a diverse array of data sources. Reviewing alerts produced by our security detection system is our primary responsibility during on-call shifts. We’re going to show you how we’re using AI…",
      "published_ts": 1764604842,
      "source_name": "Slack Engineering",
      "content_type": "technical"
    },
    {
      "url": "https://engineering.salesforce.com/how-ai-driven-refactoring-cut-a-2-year-legacy-code-migration-to-4-months/",
      "title": "How AI-Driven Refactoring Cut a 2-Year Legacy Code Migration to 4 Months",
      "summary": "By Ishay Dahan, Lilach Nachmias, and Adi Vaknin. In our Engineering Energizers Q&A series, we highlight the engineering minds driving innovation across Salesforce. Today, we meet Lilach Nachmias, Senior Manager of Software Engineering, who helped migrate the Own Archive managed package, a seven-year-old third-party application, into Salesforce’s Core infrastructure. Own is a company Salesforce acquired […] The post How AI-Driven Refactoring Cut a 2-Year Legacy Code Migration to 4 Months appeared first on Salesforce Engineering Blog .",
      "published_ts": 1764601922,
      "source_name": "Salesforce Engineering",
      "content_type": "technical"
    },
    {
      "url": "https://huggingface.co/blog/hugging-science/sarlo-80-sar-optic-language-dataset",
      "title": "SARLO-80: Worldwide Slant SAR Language Optic Dataset at 80 cm Resolution",
      "summary": "SARLO-80: Worldwide Slant SAR Language Optic Dataset at 80 cm Resolution\nAuthors: Solène Debuysère\n1\n, Nicolas Trouvé\n1\n, Nathan Letheule\n1\n, Elise Colin\n1\n, Georgia Channing\n2\nAffiliations:\n1\nONERA – The French Aerospace Lab\n2\nHugging Face\nSatellite imagery has transformed the way we observe our pl",
      "published_ts": 1764583838,
      "source_name": "Hugging Face Blog",
      "content_type": "technical"
    },
    {
      "url": "https://huggingface.co/blog/transformers-v5",
      "title": "Transformers v5: Simple model definitions powering the AI ecosystem",
      "summary": "Transformers v5: Simple model definitions powering the AI ecosystem\nTransformers' version v4.0.0rc-1, the initial release candidate for version 4, was released on November 19th, 2020. Five years later, we now release v5.0.0rc-0.\nToday, as we launch v5, Transformers is installed more than\n3 million t",
      "published_ts": 1764547200,
      "source_name": "Hugging Face Blog",
      "content_type": "technical"
    }
  ],
  "cloud_infra_observability": [
    {
      "url": "https://azure.microsoft.com/en-us/blog/new-options-for-ai-powered-innovation-resiliency-and-control-with-microsoft-azure/",
      "title": "New options for AI-powered innovation, resiliency, and control with Microsoft Azure",
      "summary": "We are extending Azure public regions with options that adapt to our customers’ evolving business requirements without forcing trade-offs. The post New options for AI-powered innovation, resiliency, and control with Microsoft Azure appeared first on Microsoft Azure Blog .",
      "published_ts": 1764781200,
      "source_name": "Azure Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/new-serverless-customization-in-amazon-sagemaker-ai-accelerates-model-fine-tuning/",
      "title": "New serverless customization in Amazon SageMaker AI accelerates model fine-tuning",
      "summary": "Accelerate AI model development with new training features that enable rapid recovery from failures and automatic scaling based on resource availability.",
      "published_ts": 1764778083,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/introducing-checkpointless-and-elastic-training-on-amazon-sagemaker-hyperpod/",
      "title": "Introducing checkpointless and elastic training on Amazon SageMaker HyperPod",
      "summary": "Accelerate AI model development with new training features that enable instant recovery from failures and automatic scaling based on resource availability.",
      "published_ts": 1764778072,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://blog.cloudflare.com/ddos-threat-report-2025-q3/",
      "title": "Cloudflare's 2025 Q3 DDoS threat report -- including Aisuru, the apex of botnets",
      "summary": "Welcome to the 23rd edition of Cloudflare’s Quarterly DDoS Threat Report. This report offers a comprehensive analysis of the evolving threat landscape of Distributed Denial of Service (DDoS) attacks based on data from the Cloudflare network. In this edition, we focus on the third quarter of 2025.",
      "published_ts": 1764770400,
      "source_name": "Cloudflare Engineering",
      "content_type": "technical"
    },
    {
      "url": "https://blog.cloudflare.com/waf-rules-react-vulnerability/",
      "title": "Cloudflare WAF proactively protects against React vulnerability",
      "summary": "Cloudflare offers protection against a new high profile vulnerability for React Server Components: CVE-2025-55182. All WAF customers are automatically protected as long as the WAF is deployed.",
      "published_ts": 1764720000,
      "source_name": "Cloudflare Engineering",
      "content_type": "technical"
    },
    {
      "url": "https://azure.microsoft.com/en-us/blog/introducing-mistral-large-3-in-microsoft-foundry-open-capable-and-ready-for-production-workloads/",
      "title": "Introducing Mistral Large 3 in Microsoft Foundry: Open, capable, and ready for production workloads",
      "summary": "Explore Mistral Large 3 in Azure—open-source, long-context, multimodal AI built for reliable enterprise workloads. The post Introducing Mistral Large 3 in Microsoft Foundry: Open, capable, and ready for production workloads appeared first on Microsoft Azure Blog .",
      "published_ts": 1764698550,
      "source_name": "Azure Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/announcing-replication-support-and-intelligent-tiering-for-amazon-s3-tables/",
      "title": "Announcing replication support and Intelligent-Tiering for Amazon S3 Tables",
      "summary": "New features enable automatic cost optimization through intelligent storage tiering and simplified table replication across AWS Regions and accounts.",
      "published_ts": 1764692354,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/amazon-s3-storage-lens-adds-performance-metrics-support-for-billions-of-prefixes-and-export-to-s3-tables/",
      "title": "Amazon S3 Storage Lens adds performance metrics, support for billions of prefixes, and export to S3 Tables",
      "summary": "New capabilities help optimize application performance, analyze unlimited prefixes, and simplify metrics analysis through S3 Tables integration.",
      "published_ts": 1764692112,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/build-multi-step-applications-and-ai-workflows-with-aws-lambda-durable-functions/",
      "title": "Build multi-step applications and AI workflows with AWS Lambda durable functions",
      "summary": "New Lambda capability lets you build applications that coordinate multiple steps reliably over extended periods—from seconds to up to one year—without paying for idle compute time when waiting for external events or human decisions.",
      "published_ts": 1764691939,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/amazon-rds-for-oracle-and-rds-for-sql-server-add-new-capabilities-to-enhance-performance-and-optimize-costs/",
      "title": "New capabilities to optimize costs and improve scalability on Amazon RDS for SQL Server and Oracle",
      "summary": "Manage development, testing, and production database workloads more efficiently with new features including Developer Edition support for SQL Server, M7i/R7i instance support with optimize CPU, and expanded storage options up to 256 TiB.",
      "published_ts": 1764691769,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/introducing-database-savings-plans-for-aws-databases/",
      "title": "Introducing Database Savings Plans for AWS Databases",
      "summary": "New pricing model helps maintain cost efficiency while providing flexibility with database services and deployment options.",
      "published_ts": 1764691766,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/amazon-cloudwatch-introduces-unified-data-management-and-analytics-for-operations-security-and-compliance/",
      "title": "Amazon CloudWatch introduces unified data management and analytics for operations, security, and compliance",
      "summary": "Reduce data management complexity and costs with automatic normalization across sources, native analytics integration, and built-in support for industry-standard formats like OCSF and Apache Iceberg.",
      "published_ts": 1764691631,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/new-and-enhanced-aws-support-plans-add-ai-capabilities-to-expert-guidance/",
      "title": "New and enhanced AWS Support plans add AI capabilities to expert guidance",
      "summary": "Prevent cloud infrastructure issues before they impact your business with AWS Support plans that combine AI-powered insights with expert guidance, offering faster response times and proactive monitoring across performance, security, and cost dimensions.",
      "published_ts": 1764691623,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/amazon-opensearch-service-improves-vector-database-performance-and-cost-with-gpu-acceleration-and-auto-optimization/",
      "title": "Amazon OpenSearch Service improves vector database performance and cost with GPU acceleration and auto-optimization",
      "summary": "Build and optimize large-scale vector databases up to 10 times faster and at a quarter of the cost with new GPU acceleration and auto-optimization capabilities that automatically balance search quality, speed, and resource usage.",
      "published_ts": 1764691601,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/amazon-s3-vectors-now-generally-available-with-increased-scale-and-performance/",
      "title": "Amazon S3 Vectors now generally available with increased scale and performance",
      "summary": "Scale vector storage and querying to new heights with S3 Vectors' general availability—now supporting up to 1 billion vectors per index, 100ms query latencies, and expanded regional availability, while reducing costs up to 90% compared to specialized databases.",
      "published_ts": 1764691571,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/introducing-amazon-ec2-x8aedz-instances-powered-by-5th-gen-amd-epyc-processors-for-memory-intensive-workloads/",
      "title": "Introducing Amazon EC2 X8aedz instances powered by 5th Gen AMD EPYC processors for memory-intensive workloads",
      "summary": "New memory-optimized instances deliver up to 5 GHz processor speeds and 3 TiB of memory—ideal for electronic design automation workloads and memory-intensive databases requiring high single-threaded performance.",
      "published_ts": 1764691544,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/aws-devops-agent-helps-you-accelerate-incident-response-and-improve-system-reliability-preview/",
      "title": "AWS DevOps Agent helps you accelerate incident response and improve system reliability (preview)",
      "summary": "New service acts as an always-on DevOps engineer, helping you respond to incidents, identify root causes, and prevent future issues through systematic analysis of incidents and operational patterns.",
      "published_ts": 1764691542,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/amazon-fsx-for-netapp-ontap-now-integrates-with-amazon-s3-for-seamless-data-access/",
      "title": "Amazon FSx for NetApp ONTAP now integrates with Amazon S3 for seamless data access",
      "summary": "Access FSx for NetApp ONTAP file data through S3 to enable AI/ML workloads and analytics—letting you use enterprise file data with Bedrock, SageMaker, and analytics services while it remains in your file system.",
      "published_ts": 1764691194,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/new-aws-security-agent-secures-applications-proactively-from-design-to-deployment-preview/",
      "title": "New AWS Security Agent secures applications proactively from design to deployment (preview)",
      "summary": "Scale your AppSec expertise with AI-powered design reviews, code analysis, and contextual penetration testing that understand your unique security requirements and application architecture.",
      "published_ts": 1764691121,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/aws-security-hub-now-generally-available-with-near-real-time-analytics-and-risk-prioritization/",
      "title": "AWS Security Hub now generally available with near real-time analytics and risk prioritization",
      "summary": "Today, AWS Security Hub is generally available, transforming how security teams identify and respond to critical security risks across their AWS environments. These new capabilities were first announced in preview at AWS re:Inforce 2025. Security Hub prioritizes your critical security issues and unifies your security operations to help you respond at scale by correlating and […]",
      "published_ts": 1764691091,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/amazon-guardduty-adds-extended-threat-detection-for-amazon-ec2-and-amazon-ecs/",
      "title": "Amazon GuardDuty adds Extended Threat Detection for Amazon EC2 and Amazon ECS",
      "summary": "Today, we’re announcing new enhancements to Amazon GuardDuty Extended Threat Detection with the addition of two attack sequence findings for Amazon Elastic Compute Cloud (Amazon EC2) instances and Amazon Elastic Container Service (Amazon ECS) tasks. These new findings build on the existing Extended Threat Detection capabilities, which already combine sequences involving AWS Identity and Access […]",
      "published_ts": 1764691074,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://www.uber.com/blog/improving-mysql-cluster-uptime-part1/",
      "title": "Improving MySQL® Cluster Uptime: Designing Advanced Detection, Mitigation, and Consensus with Group Replication",
      "summary": "At Uber, high availability is non-negotiable. Learn how we’ve adopted MySQL® Group Replication in single-primary mode to achieve a less than 10 second failover time  and massively improve reliability and write availability during failures.",
      "published_ts": 1764684000,
      "source_name": "Uber Engineering Blog",
      "content_type": "technical"
    },
    {
      "url": "https://stripe.com/blog/new-features-to-help-saas-platforms-manage-risk-and-stay-compliant",
      "title": "New features to help SaaS platforms manage risk and stay compliant",
      "summary": "We recently launched three new features that give you more control to fine-tune your risk and compliance strategy, allowing you to use more of Stripe’s data to inform your approach. Here’s what’s new.",
      "published_ts": 1764633600,
      "source_name": "Stripe Engineering Blog",
      "content_type": "technical"
    },
    {
      "url": "https://about.gitlab.com/blog/automate-embedded-systems-compliance-with-gitlab-and-codesonar/",
      "title": "Automate embedded systems compliance with GitLab and CodeSonar",
      "summary": "Embedded systems development teams face a persistent challenge: maintaining development velocity while meeting stringent functional safety and code quality requirements. Standards like ISO 26262, IEC 62304, DO-178C, and IEC 61508 demand rigorous verification processes that are often manual and time-consuming. Compliance reviews against coding standards like MISRA C/C++, isolated scanning workflows, and post-development verification create bottlenecks. Teams are forced to choose between speed and safety. GitLab's integration with CodeSonar (from AdaCore) addresses this challenge by automating compliance workflows and enabling continuous verification throughout the development lifecycle. Specialized scanning for safety-critical systems Safety-critical systems require deep analysis of C/C++ code compiled with specialized embedded tools. These systems must demonstrate compliance with coding standards (MISRA C/C++, CERT C/C++, AUTOSAR C++) and functional safety frameworks (ISO 26262, DO-178C, IEC 61508) that require detailed evidence trails. Beyond aligning with coding standards, teams also need to address security concerns. This means testing for memory problems as well as a host of other problems like uninitialized variables and command injection. CodeSonar performs whole program analysis with specialized scanning capabilities for these standards. Pairing CodeSonar with GitLab enables teams to automate compliance workflows and maintain comprehensive audit trails throughout the development lifecycle. Automating compliance from commit to merge The GitLab and CodeSonar integration provides a compliance-as-code approach that automates policy enforcement from the earliest stages of development. CodeSonar functions as an additional scanner within GitLab CI/CD pipelines , analyzing code in every commit and merge request. Because CodeSonar was purpose-built for embedded systems, it performs deep control flow and data flow analysis across entire programs, identifying vulnerabilities like buffer overruns, data taint, uninitialized variables, use-after-free conditions, and command injection — the root causes of most security incidents in embedded systems. The integration works through GitLab's CI/CD configuration. When developers push code changes, the pipeline triggers CodeSonar scanning. For C and C++ firmware, CodeSonar observes compiler invocations during the actual build process, creating an internal representation of the code that enables sophisticated analysis. Results are converted from SARIF format to GitLab's Static Application Security Testing ( SAST ) format and surfaced directly in merge requests, where they feed into GitLab Ultimate's Security Dashboard, Vulnerability Management, and Compliance Frameworks . Example workflow: ISO 26262 ASIL-D compliance The demo video below shows the complete workflow for an embedded system subject to ISO 26262 ASIL-D requirements. The scenario illustrates how embedded development teams can implement continuous compliance without compromising development velocity. <div style=\"padding:56.25% 0 0 0;position:relative;\"><iframe src=\"https://player.vimeo.com/video/1139086924?badge=0&autopause=0&player_id=0&app_id=58479\" frameborder=\"0\" allow=\"autoplay; fullscreen; picture-in-picture; clipboard-write; encrypted-media; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" title=\"Automated Compliance for Embedded Systems using GitLab and CodeSonar\"></iframe></div><script src=\"https://player.vimeo.com/api/player.js\"></script>` The workflow begins with a developer submitting a merge request for firmware changes. GitLab's CI/CD pipeline automatically triggers CodeSonar scanning, which performs deep C/C++ analysis against custom ISO 26262 policies configured in the pipeline. When CodeSonar identifies an ASIL-D relevant vulnerability, the pipeline halts automatically per the compliance policy, with clear documentation explaining the issue. The complete scan results, issue tracking, and approval workflow are maintained in GitLab as a single source of truth for audit trails. Developers can use both the CodeSonar hub interface and GitLab Duo AI to understand the vulnerability. CodeSonar provides detailed information about the path through the source code that leads to the problem, along with code navigation features to isolate the root cause. GitLab Duo explains the vulnerability and provides specific remediation recommendations . After the developer implements the fix and validates the resolution, the code merges successfully with full compliance evidence automatically collected throughout the process. Benefits of the integration Organizations implementing this integrated compliance with GitLab and CodeSonar will see significant improvements in both development velocity and compliance confidence. Efficiency gains: Development teams reduce time-to-market by catching coding standard compliance issues early when they're less expensive to fix. Automated security policy enforcement decreases manual security review overhead, freeing specialists to focus on complex problems rather than routine checks. Audit readiness improves through automated evidence collection. Compliance artifacts are generated as a by-product of normal development rather than through separate documentation efforts. Compliance maturity: This integrated approach helps organizations maintain continuous compliance with industry standards and regulations. By embedding verification into every code change, teams build comprehensive audit trails that demonstrate adherence to ISO 26262, DO-178C, MISRA C/C++, and other requirements. The automated workflow transforms compliance from a periodic checkpoint into an ongoing verification process. Implementation considerations Implementing the GitLab and CodeSonar integration requires access to GitLab Ultimate, a CodeSonar hub, GitLab runners where code can be compiled and analyzed, and appropriate mechanisms for managing analysis data files. Both GitLab and CodeSonar fully support on-premises and air-gapped environments and can be deployed to auto-scalable cloud environments as well. Teams should configure Custom Compliance Frameworks in GitLab to define specific policies for their relevant standards: ISO 26262 for automotive, DO-178C for aerospace, IEC 62304 for medical devices, and others. These frameworks enable automated enforcement of compliance requirements through merge request approval rules, vulnerability thresholds, and scan policy gates. Get started The CodeSonar GitLab CI component is available through GitLab's CI/CD Catalog. Detailed integration documentation provides platform-specific setup instructions for Linux, Docker, and Windows environments. For organizations evaluating this solution, the implementation demonstrates how specialized embedded systems tools can integrate with a modern DevSecOps platform to deliver both development velocity and compliance rigor. For more information about implementing GitLab with CodeSonar for your embedded systems development, visit the CodeSonar integration documentation . You can also request a trial of CodeSonar .",
      "published_ts": 1764633600,
      "source_name": "GitLab Engineering",
      "content_type": "technical"
    }
  ],
  "data_modeling_governance": [
    {
      "url": "https://seattledataguy.substack.com/p/translating-data-buzzwords-into-real",
      "title": "Translating Data Buzzwords Into Real Requirements",
      "summary": "Bridging the Communication Gap Between Data and the Business",
      "published_ts": 1764688343,
      "source_name": "Seattle Data Guy",
      "content_type": "technical"
    },
    {
      "url": "https://www.databricks.com/blog/completing-lakehouse-vision-open-storage-open-access-unified-governance",
      "title": "Completing the Lakehouse Vision: Open Storage, Open Access, Unified Governance",
      "summary": "Until now, organizations have had no way to unify attribute-based access control...",
      "published_ts": 1764683680,
      "source_name": "Databricks Blog",
      "content_type": "technical"
    }
  ],
  "etl_orchestration": [
    {
      "url": "https://dagster.io/blog/orchestrating-nanochat-building-the-tokenizer",
      "title": "From NanoChat to Dagster: Building Your Ingestion Pipeline and Custom Tokenizer",
      "summary": "Explore how to transform NanoChat into a Dagster-powered LLM system by designing ingestion flows, cleaning datasets, and building a tokenizer tailored to your model’s needs.",
      "published_ts": 1764792629,
      "source_name": "Dagster Blog",
      "content_type": "technical"
    },
    {
      "url": "https://about.gitlab.com/blog/migration-from-azure-devops-to-gitlab/",
      "title": "Guide: Migrate from Azure DevOps to GitLab",
      "summary": "Migrating from Azure DevOps to GitLab can seem like a daunting task, but with the right approach and tools, it can be a smooth and efficient process. This guide will walk you through the steps needed to successfully migrate your projects, repositories, and pipelines from Azure DevOps to GitLab. Overview GitLab provides both Congregate (maintained by GitLab Professional Services organization) and a built-in Git repository import for migrating projects from Azure DevOps (ADO). These options support repository-by-repository or bulk migration and preserve git commit history, branches, and tags. With Congregate and professional services tools, we support additional assets such as wikis, work items, CI/CD variables, container images, packages, pipelines, and more (see this feature matrix ). Use this guide to plan and execute your migration and complete post-migration follow-up tasks. Enterprises migrating from ADO to GitLab commonly follow a multi-phase approach: Migrate repositories from ADO to GitLab using Congregate or GitLab's built-in repository migration. Migrate pipelines from Azure Pipelines to GitLab CI/CD. Migrate remaining assets such as boards, work items, and artifacts to GitLab Issues, Epics, and the Package and Container Registries. High-level migration phases: graph LR\n    subgraph Prerequisites\n        direction TB\n        A[\"Set up identity provider (IdP) and<br/>provision users\"]\n        A --> B[\"Set up runners and<br/>third-party integrations\"]\n        B --> I[\"Users enablement and<br/>change management\"]\n    end\n    \n    subgraph MigrationPhase[\"Migration phase\"]\n        direction TB\n        C[\"Migrate source code\"]\n        C --> D[\"Preserve contributions and<br/> format history\"]\n        D --> E[\"Migrate work items and<br/>map to <a href=\"https://docs.gitlab.com/topics/plan_and_track/\">GitLab Plan <br/>and track work\"]\n    end\n    \n    subgraph PostMigration[\"Post-migration steps\"]\n        direction TB\n        F[\"Create or translate <br/>ADO pipelines to GitLab CI\"]\n        F --> G[\"Migrate other assets<br/>packages and container images\"]\n        G --> H[\"Introduce <a href=\"https://docs.gitlab.com/user/application_security/secure_your_application/\">security</a> and<br/>SDLC improvements\"]\n    end\n    \n    Prerequisites --> MigrationPhase\n    MigrationPhase --> PostMigration\n\n    style A fill:#FC6D26\n    style B fill:#FC6D26\n    style I fill:#FC6D26\n    style C fill:#8C929D\n    style D fill:#8C929D\n    style E fill:#8C929D\n    style F fill:#FFA500\n    style G fill:#FFA500\n    style H fill:#FFA500 Planning your migration To plan your migration, ask these questions: How soon do we need to complete the migration? Do we understand what will be migrated? Who will run the migration? What organizational structure do we want in GitLab? Are there any constraints, limitations, or pitfalls that need to be taken into account? Determine your timeline, as it will largely dictate your migration approach. Identify champions or groups familiar with both ADO and GitLab platforms (such as early adopters) to help drive adoption and provide guidance. Inventory what you need to migrate: The number of repositories, pull requests, and contributors The number and complexity of work items and pipelines Repository sizes and dependency relationships Critical integrations and runner requirements (agent pools with specific capabilities) Use GitLab Professional Services's Evaluate tool to produce a complete inventory of your entire Azure DevOps organization, including repositories, PR counts, contributor lists, number of pipelines, work items, CI/CD variables and more. If you're working with the GitLab Professional Services team, share this report with your engagement manager or technical architect to help plan the migration. Migration timing is primarily driven by pull request count, repository size, and amount of contributions (e.g. comments in PR, work items, etc). For example, 1,000 small repositories with few PRs and limited contributors can migrate much faster than a smaller set of repositories containing tens of thousands of PRs and thousands of contributors. Use your inventory data to estimate effort and plan test runs before proceeding with production migrations. Compare inventory against your desired timeline and decide whether to migrate all repositories at once or in batches. If teams cannot migrate simultaneously, batch and stagger migrations to align with team schedules. For example, in Professional Services engagements, we organize migrations into waves of 200-300 projects to manage complexity and respect API rate limits, both in GitLab and ADO . GitLab's built-in repository importer migrates Git repositories (commits, branches, and tags) one-by-one. Congregate is designed to preserve pull requests (known in GitLab as merge requests), comments, and related metadata where possible; the simple built-in repository import focuses only on the Git data (history, branches, and tags). Items that typically require separate migration or manual recreation: Azure Pipelines - create equivalent GitLab CI/CD pipelines (consult with CI/CD YAML and/or with CI/CD components ). Alternatively, consider using AI-based pipeline conversion available in Congregate. Work items and boards - map to GitLab Issues, Epics, and Issue Boards. Artifacts, container images (ACR) - migrate to GitLab Package Registry or Container Registry. Service hooks and external integrations - recreate in GitLab. Permissions models differ between ADO and GitLab; review and plan permissions mapping rather than assuming exact preservation. Review what each tool (Congregate vs. built-in import) will migrate and choose the one that fits your needs. Make a list of any data or integrations that must be migrated or recreated manually. Who will run the migration? Migrations are typically run by a GitLab group owner or instance administrator, or by a designated migrator who has been granted the necessary permissions on the destination group/project. Congregate and the GitLab import APIs require valid authentication tokens for both Azure DevOps and GitLab. Decide whether a group owner/admin will perform the migrations or whether you will grant a specific team/person delegated access. Ensure the migrator has correctly configured personal access tokens (Azure DevOps and GitLab) with the scopes required by your chosen migration tool (for example, api/read_repository scopes and any tool-specific requirements). Test tokens and permissions with a small pilot migration. Note: Congregate leverages file-based import functionality for ADO migrations and requires instance administrator permissions to run ( see our documentation ). If you are migrating to GitLab.com, consider engaging Professional Services. For more information, see the Professional Services Full Catalog . Non-admin account cannot preserve contribution attribution! What organizational structure do we want in GitLab? While it's possible to map ADO structure directly to GitLab structure, it's recommended to rationalize and simplify the structure during migration. Consider how teams will work in GitLab and design the structure to facilitate collaboration and access management. Here is a way to think about mapping ADO structure to GitLab structure: graph TD\n    subgraph GitLab\n        direction TB\n        A[\"Top-level Group\"]\n        B[\"Subgroup (optional)\"]\n        C[\"Projects\"]\n        A --> B\n        A --> C\n        B --> C\n    end\n\n    subgraph AzureDevOps[\"Azure DevOps\"]\n        direction TB\n        F[\"Organizations\"]\n        G[\"Projects\"]\n        H[\"Repositories\"]\n        F --> G\n        G --> H\n    end\n\n    style A fill:#FC6D26\n    style B fill:#FC6D26\n    style C fill:#FC6D26\n    style F fill:#8C929D\n    style G fill:#8C929D\n    style H fill:#8C929D Recommended approach: Map each ADO organization to a GitLab group (or a small set of groups), not to many small groups. Avoid creating a GitLab group for every ADO team project. Use migration as an opportunity to rationalize your GitLab structure. Use subgroups and project-level permissions to group related repositories. Manage access to sets of projects by using GitLab groups and group membership (groups and subgroups) rather than one group per team project. Review GitLab permissions and consider SAML Group Links to implement an enterprise RBAC model for your GitLab instance (or a GitLab.com namespace). ADO Boards and work items: State of migration It's important to understand how work items migrate from ADO into GitLab Plan (issues, epics, and boards). ADO Boards and work items map to GitLab Issues, Epics, and Issue Boards. Plan how your workflows and board configurations will translate. ADO Epics and Features become GitLab Epics. Other work item types (e.g., user stories, tasks, bugs) become project-scoped issues. Most standard fields are preserved; selected custom fields can be migrated when supported. Parent-child relationships are retained so Epics reference all related issues. Links to pull requests are converted to merge request links to maintain development traceability. Example: Migration of an individual work item to a GitLab Issue, including field accuracy and relationships: Batching guidance: If you need to run migrations in batches, use your new group/subgroup structure to define batches (for example, by ADO organization or by product area). Use inventory reports to drive batch selection and test each batch with a pilot migration before scaling. Pipelines migration Congregate recently introduced AI-powered conversion for multi-stage YAML pipelines from Azure DevOps to GitLab CI/CD. This automated conversion works best for simple, single-file pipelines and is designed to provide a working starting point rather than a production-ready .gitlab-ci.yml file. The tool generates a functionally equivalent GitLab pipeline that you can then refine and optimize for your specific needs. Converts Azure Pipelines YAML to .gitlab-ci.yml format automatically. Best suited for straightforward, single-file pipeline configurations. Provides a boilerplate to accelerate migration, not a final production artifact. Requires review and adjustment for complex scenarios, custom tasks, or enterprise requirements. Does not support Azure DevOps classic release pipelines — convert these to multi-stage YAML first. Repository owners should review the GitLab CI/CD documentation to further optimize and enhance their pipelines after the initial conversion. Example of converted pipelines: # azure-pipelines.yml\n\ntrigger:\n  - main\n\nvariables:\n  imageName: myapp\n\nstages:\n  - stage: Build\n    jobs:\n      - job: Build\n        pool:\n          vmImage: 'ubuntu-latest'\n        steps:\n          - checkout: self\n\n          - task: Docker@2\n            displayName: Build Docker image\n            inputs:\n              command: build\n              repository: $(imageName)\n              Dockerfile: '**/Dockerfile'\n              tags: |\n                $(Build.BuildId)\n\n  - stage: Test\n    jobs:\n      - job: Test\n        pool:\n          vmImage: 'ubuntu-latest'\n        steps:\n          - checkout: self\n\n          # Example: run tests inside the container\n          - script: |\n              docker run --rm $(imageName):$(Build.BuildId) npm test\n            displayName: Run tests\n\n  - stage: Push\n    jobs:\n      - job: Push\n        pool:\n          vmImage: 'ubuntu-latest'\n        steps:\n          - checkout: self\n\n          - task: Docker@2\n            displayName: Login to ACR\n            inputs:\n              command: login\n              containerRegistry: '<your-acr-service-connection>'\n\n          - task: Docker@2\n            displayName: Push image to ACR\n            inputs:\n              command: push\n              repository: $(imageName)\n              tags: |\n                $(Build.BuildId) # .gitlab-ci.yml\n\nvariables:\n  imageName: myapp\n\nstages:\n  - build\n  - test\n  - push\n\nbuild:\n  stage: build\n  image: docker:latest\n  services:\n    - docker:dind\n  script:\n    - docker build -t $imageName:$CI_PIPELINE_ID -f $(find . -name Dockerfile) .\n  only:\n    - main\n\ntest:\n  stage: test\n  image: docker:latest\n  services:\n    - docker:dind\n  script:\n    - docker run --rm $imageName:$CI_PIPELINE_ID npm test\n  only:\n    - main\n\npush:\n  stage: push\n  image: docker:latest\n  services:\n    - docker:dind\n  before_script:\n    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY\n  script:\n    - docker tag $imageName:$CI_PIPELINE_ID $CI_REGISTRY/$CI_PROJECT_PATH/$imageName:$CI_PIPELINE_ID\n    - docker push $CI_REGISTRY/$CI_PROJECT_PATH/$imageName:$CI_PIPELINE_ID\n  only:\n    - main Final checklist: Decide timeline and batch strategy. Produce a full inventory of repositories, PRs, and contributors. Choose Congregate or the built-in import based on scope (PRs and metadata vs. Git data only). Decide who will run migrations and ensure tokens/permissions are configured. Identify assets that must be migrated separately (pipelines, work items, artifacts, and hooks) and plan those efforts. Run pilot migrations, validate results, then scale according to your plan. Running your migrations After planning, execute migrations in stages, starting with trial runs. Trial migrations help surface org-specific issues early and let you measure duration, validate outcomes, and fine-tune your approach before production. What trial migrations validate: Whether a given repository and related assets migrate successfully (history, branches, tags; plus MRs/comments if using Congregate) Whether the destination is usable immediately (permissions, runners, CI/CD variables, integrations) How long each batch takes, to set schedules and stakeholder expectations Downtime guidance: GitLab's built-in Git import and Congregate do not inherently require downtime. For production waves, freeze changes in ADO (branch protections or read-only) to avoid missed commits, PR updates, or work items created mid-migration. Trial runs do not require freezes and can be run anytime. Batching guidance: Run trial batches back-to-back to shorten elapsed time; let teams validate results asynchronously. Use your planned group/subgroup structure to define batches and respect API rate limits. Recommended steps: Create a test destination in GitLab for trials: GitLab.com: create a dedicated group/namespace (for example, my-org-sandbox) Self-managed: create a top-level group or a separate test instance if needed Prepare authentication: Azure DevOps PAT with required scopes. GitLab Personal Access Token with api and read_repository (plus admin access for file-based imports used by Congregate). Run trial migrations: Repos only: use GitLab's built-in import (Repo by URL) Repos + PRs/MRs and additional assets: use Congregate Post-trial follow-up: Verify repo history, branches, tags; merge requests (if migrated), issues/epics (if migrated), labels, and relationships. Check permissions/roles, protected branches, required approvals, runners/tags, variables/secrets, integrations/webhooks. Validate pipelines ( .gitlab-ci.yml ) or converted pipelines where applicable. Ask users to validate functionality and data fidelity. Resolve issues uncovered during trials and update your runbooks. Network and security: If your destination uses IP allow lists, add the IPs of your migration host and any required runners/integrations so imports can succeed. Run production migrations in waves: Enforce change freezes in ADO during each wave. Monitor progress and logs; retry or adjust batch sizes if you hit rate limits. Optional: remove the sandbox group or archive it after you finish. <figure class=\"video_container\">\n<iframe src=\"https://www.youtube.com/embed/ibIXGfrVbi4?si=ZxOVnXjCF-h4Ne0N\" frameborder=\"0\" allowfullscreen=\"true\"></iframe>\n</figure> Terminology reference for GitLab and Azure DevOps GitLab Azure DevOps Similarities & Key Differences Group Organization Top-level namespace, membership, policies. ADO org contains Projects; GitLab Group contains Subgroups and Projects. Group or Subgroup Project Logical container, permissions boundary. ADO Project holds many repos; GitLab Groups/Subgroups organize many Projects. Project (includes a Git repo) Repository (inside a Project) Git history, branches, tags. In GitLab, a \"Project\" is the repo plus issues, CI/CD, wiki, etc. One repo per Project. Merge Request (MR) Pull Request (PR) Code review, discussions, approvals. MR rules include approvals, required pipelines, code owners. Protected Branches, MR Approval Rules, Status Checks Branch Policies Enforce reviews and checks. GitLab combines protections + approval rules + required status checks. GitLab CI/CD Azure Pipelines YAML pipelines, stages/jobs, logs. ADO also has classic UI pipelines; GitLab centers on .gitlab-ci.yml. .gitlab-ci.yml azure-pipelines.yml Defines stages/jobs/triggers. Syntax/features differ; map jobs, variables, artifacts, and triggers. Runners (shared/specific) Agents / Agent Pools Execute jobs on machines/containers. Target via demands (ADO) vs tags (GitLab). Registration/scoping differs. CI/CD Variables (project/group/instance), Protected/Masked Pipeline Variables, Variable Groups, Library Pass config/secrets to jobs. GitLab supports group inheritance and masking/protection flags. Integrations, CI/CD Variables, Deploy Keys Service Connections External auth to services/clouds. Map to integrations or variables; cloud-specific helpers available. Environments & Deployments (protected envs) Environments (with approvals) Track deploy targets/history. Approvals via protected envs and manual jobs in GitLab. Releases (tag + notes) Releases (classic or pipelines) Versioned notes/artifacts. GitLab Release ties to tags; deployments tracked separately. Job Artifacts Pipeline Artifacts Persist job outputs. Retention/expiry configured per job or project. Package Registry (NuGet/npm/Maven/PyPI/Composer, etc.) Azure Artifacts (NuGet/npm/Maven, etc.) Package hosting. Auth/namespace differ; migrate per package type. GitLab Container Registry Azure Container Registry (ACR) or others OCI images. GitLab provides per-project/group registries. Issue Boards Boards Visualize work by columns. GitLab boards are label-driven; multiple boards per project/group. Issues (types/labels), Epics Work Items (User Story/Bug/Task) Track units of work. Map ADO types/fields to labels/custom fields; epics at group level. Epics, Parent/Child Issues Epics/Features Hierarchy of work. Schema differs; use epics + issue relationships. Milestones and Iterations Iteration Paths Time-boxing. GitLab Iterations (group feature) or Milestones per project/group. Labels (scoped labels) Area Paths Categorization/ownership. Replace hierarchical areas with scoped labels. Project/Group Wiki Project Wiki Markdown wiki. Backed by repos in both; layout/auth differ slightly. Test reports via CI, Requirements/Test Management, integrations Test Plans/Cases/Runs QA evidence/traceability. No 1:1 with ADO Test Plans; often use CI reports + issues/requirements. Roles (Owner/Maintainer/Developer/Reporter/Guest) + custom roles Access levels + granular permissions Control read/write/admin. Models differ; leverage group inheritance and protected resources. Webhooks Service Hooks Event-driven integrations. Event names/payloads differ; reconfigure endpoints. Advanced Search Code Search Full-text repo search. Self-managed GitLab may need Elasticsearch/OpenSearch for advanced features.",
      "published_ts": 1764720000,
      "source_name": "GitLab Engineering",
      "content_type": "technical"
    },
    {
      "url": "https://airbyte.com/blog/agentic-data-infrastructure",
      "title": "The Rise of Agentic Data Infrastructure | Airbyte",
      "summary": "Discover how agentic data infrastructure is transforming modern data systems by combining automation, intelligence, and adaptability.",
      "published_ts": 1764720000,
      "source_name": "Airbyte Blog",
      "content_type": "technical"
    },
    {
      "url": "https://vutr.substack.com/p/my-framework-to-build-a-data-pipeline",
      "title": "A framework I use to build a data pipeline.",
      "summary": "You cannot simply say, \"I'll use Spark, Kafka, and so on\"; you need to ask clarifying questions to gather information for proposing a robust data pipeline.",
      "published_ts": 1764645335,
      "source_name": "VuTrinh · Data Engineering",
      "content_type": "technical"
    },
    {
      "url": "https://www.databricks.com/blog/events-insights-complex-state-processing-schema-evolution-transformwithstate",
      "title": "From Events to Insights: Complex State Processing with Schema Evolution in transformWithState",
      "summary": "Across industries, one of the most persistent challenges in data engineering is schema...",
      "published_ts": 1764608444,
      "source_name": "Databricks Blog",
      "content_type": "technical"
    },
    {
      "url": "https://airbyte.com/blog/introducing-agent-blueprint",
      "title": "Introducing Agent Blueprint | Airbyte",
      "summary": "Agent Blueprint helps teams connect real-time data to AI agents with production-ready infrastructure, faster integration, and reliable context management.",
      "published_ts": 1764547200,
      "source_name": "Airbyte Blog",
      "content_type": "technical"
    },
    {
      "url": "https://about.gitlab.com/blog/continuously-deploying-the-largest-gitlab-instance/",
      "title": "How we deploy the largest GitLab instance 12 times daily",
      "summary": "Every day, GitLab deploys code changes to the world's largest GitLab instance — GitLab.com  — up to 12 times without any downtime. We use GitLab's own CI/CD platform to manage these deployments, which impact millions of developers worldwide. This deployment frequency serves as our primary quality gate and stress test. It also means our customers get access to new features within hours of development rather than waiting weeks or months. When organizations depend on GitLab for their DevOps workflows, they're using a platform that's proven at scale on our own infrastructure. In this article, you'll learn how we built an automated deployment pipeline using core GitLab CI/CD functionality to handle this deployment complexity. The business case for deployment velocity For GitLab: Our deployment frequency isn't just an engineering metric — it's a business imperative. Rapid deployment cycles mean we can respond to customer feedback within hours, ship security patches immediately, and validate new features in production before scaling them. For our customers: Every deployment to GitLab.com validates the deployment practices we recommend to our users. When you use GitLab's deployment features, you're using the same battle-tested approach that handles millions of git operations, CI/CD pipelines, and user interactions daily. You benefit from: Latest features available immediately: New capabilities reach you within hours of completion, not in quarterly release cycles Proven reliability at scale: If a feature works on GitLab.com, you can trust it in your environment Full value of GitLab: Zero-downtime deployments mean you never lose access to your DevOps platform, even during updates Real-world tested practices: Our deployment documentation isn't theory — it's exactly how we run the largest GitLab instance in existence Code flow architecture Our deployment pipeline follows a structured progression through multiple stages, each acting as a checkpoint on the journey from code proposal to production deployment. graph TD\n      A[Code Proposed] --> B[Merge Request Created]\n      B --> C[Pipeline Triggered]\n      C --> D[Build & Test]\n      D --> E{Spec/Integration/QA Tests Pass?}\n      E -->|No| F[Feedback Loop]\n      F --> B\n      E -->|Yes| G[Merge to default branch]\n      G -->|Periodically| H[Auto-Deploy Branch]\n\n      subgraph \"Deployment Pipeline\"\n          H --> I[Package Creation]\n          I --> K[Canary Environment]\n          K --> L[QA Validation]\n          L --> M[Main Environment]\n\n      end Deployment pipeline makeup Our deployment approach uses GitLab's native CI/CD capabilities to orchestrate complex deployments across hybrid infrastructure.\nHere's how we do it. Build Building GitLab is a complex topic in and of itself, so I'll go over the details at a high level. We build both our Omnibus package and our Cloud Native GitLab (CNG) images. The Omnibus packages deploy to our Gitaly fleet (our Git storage layer), while CNG images run all other components as containerized workloads. Other stateful services like Postgres and Redis have grown so large we have dedicated teams managing them separately. For GitLab.com, those systems are not deployed during our Auto-Deploy procedures. We have a scheduled pipeline that will regularly look at gitlab-org/gitlab and search for the most recent commit on the default branch with a successful (“green”) pipeline. Green pipelines signal that every component of GitLab has passed its comprehensive test suite. We then create an auto-deploy branch from that commit. This triggers a sequence of events: primarily, the need to build this package and all components that are a part of our monolith.\nAnother scheduled pipeline selects the latest built package and initiates the deployment pipeline. Procedurally, it looks this simple: graph LR\n      A[Create branch] --> B[Build]\n      B --> C[Choose Built package]\n      C --> D[Start Deploy Pipeline] Building takes some time, and since deployments can vary due to various circumstances, we choose the latest build to deploy. We technically build more versions of GitLab for .com than will ever be deployed. This enables us to always have a package lined up ready to go, and this brings us the closest we can be to having a full continuously delivered product for .com. Environment-based validation and Canary strategy Quality assurance (QA) isn't just an afterthought here — it's baked into every layer from development through deployment. Our QA process leverages automated test suites that include unit tests, integration tests, and end-to-end tests that simulate real user interactions with GitLab's features. But more importantly for our deployment pipeline, our QA process works hand-in-hand with our Canary strategy through environment-based validation. As part of our validation approach, we leverage GitLab's native Canary deployments , enabling controlled validation of changes with limited traffic exposure before full production deployment. We send roughly 5% of all traffic through our Canary stage . This approach increases the complexity of database migrations, but successfully navigating Canary deployments ensures we deploy a reliable product seamlessly. The Canary deployment features you use in GitLab were refined through managing one of the most complex deployment scenarios in production. When you implement Canary deployments for your applications, you're using patterns proven at massive scale. Our deployment process follows a progressive rollout strategy: Staging Canary: Initial validation environment Production Canary: Limited production traffic Staging Main: Full staging environment deployment Production Main: Full production rollout graph TD\n      C[Staging Canary Deploy]\n      C --> D[QA Smoke Main Stage Tests]\n      C --> E[QA Smoke Canary Stage Tests]\n      D --> F\n      E --> F{Tests Pass?}\n      F -->|Yes| G[Production Canary Deploy]\n      G --> S[QA Smoke Main Stage Tests]\n      G --> T[QA Smoke Canary Stage Tests]\n      F -->|No| H[Issue Creation]\n      H --> K[Fix & Backport]\n      K --> C\n\n      S --> M[Canary Traffic Monitoring]\n      T --> M[Canary Traffic Monitoring baking period]\n      M --> U[Production Safety Checks]\n      U --> N[Staging Main]\n      N --> V[Production Main] Our QA validation occurs at multiple checkpoints throughout this progressive deployment process: after each Canary deployment, and again after post-deploy migrations. This multilayered approach ensures that each phase of our deployment strategy has its own safety net. You can learn more about GitLab's comprehensive testing approach in our handbook. Deployment pipeline Here are the challenges we address across our deployment pipeline. Technical architecture considerations GitLab.com represents real-world deployment complexity at scale. As the largest known GitLab instance, deployments use our official GitLab Helm chart and the official Linux package — the same artifacts our customers use. You can learn more about the GitLab.com architecture in our handbook. This hybrid approach means our deployment pipeline must intelligently handle both containerized services and traditional Linux services in the same deployment cycle. Dogfooding at scale: We deploy using the same procedures we document for zero-downtime upgrades . If something doesn't work smoothly for us, we don't recommend it to customers. This self-imposed constraint drives continuous improvement in our deployment tooling. The following stages are run for all environment and stage upgrades: graph LR\n      a[prep] --> c[Regular Migrations - Canary stage only]\n      a --> f[Assets - Canary stage only]\n      c --> d[Gitaly]\n      d --> k8s\n\n      subgraph subGraph0[\"VM workloads\"]\n        d[\"Gitaly\"]\n      end\n\n      subgraph subGraph1[\"Kubernetes workloads\"]\n        k8s[\"k8s\"]\n      end\n\n      subgraph fleet[\"fleet\"]\n        subGraph0\n        subGraph1\n      end Stage details: Prep: Validates deployment readiness and performs pre-deployment checks Migrations: Executes database regular migrations. This only happens during the Canary stage. Because both Canary and Main stages share the same database, these changes are already available when the Main stage deploys, eliminating the need to repeat these tasks. Assets: We leverage a GCS bucket for all static assets. If any new assets are created, we upload these to our bucket such that they are immediately available to our Canary stage. As we leverage WebPack for assets, and properly leverage SHAs in the naming of our assets, we can confidently not worry that we override an older asset. Therefore, old assets continue to be available for older deployments and new assets are imemdiately made available when Canary begins its deploy. This only happens during the Canary stage deployment. Because Canary and Main stages share the same asset storage, these changes are already available when the Main stage deploys. Gitaly: Updates Gitaly Virtual Machine storage layer via our Omnibus Linux package on each Gitaly node. This service is unique as we bundle it with git . Therefore, we need to ensure that this service is capable of atomic upgrades. We leverage a wrapper around Gitaly , which enables us to install a newer version of Gitaly and make use of the library tableflip to cleanly rotate the running Gitaly, ensuring high availability of this service on each of our instances. Kubernetes: Deploys containerized GitLab components via our Helm chart. Note that we deploy to numerous clusters spread across Zones for redundancy, so these are usually broken into their own stages to minimize harm and sometimes allows us to stop mid-deploy if critical issues are detected. Multi-version compatibility: The hidden challenge As you read our process, you will notice that there's a period of time where our database schema is ahead of the code that the Main stage knows about. This happens because the Canary stage has already deployed new code and runs regular database migrations, but the Main stage is still running the previous version of the code that doesn't know about these new database changes. Real-world example: Imagine we're adding a new merge_readiness field to merge requests. During deployment, some servers are running code that expects this field. while others don't know it exists yet. If we handle this poorly, we break GitLab.com for millions of users. If we handle it well, nobody notices anything happened. This occurs with most other services, as well. For example, if a client sends multiple requests, there's a chance one of them might land in our Canary stage; other requests might be directed to the Main stage. This is not too different from a deploy as it does take a decent amount of time to roll through the few thousand Pods that run our services. With a few exceptions, the vast majority of our services will run a slightly newer version of that component in Canary for a period of time. In a sense, these scenarios are all transient states. But they can often persist for several hours or days in a live, production environment. Therefore, we must treat them with the same care as permanent states. During any deployment, we have multiple versions of GitLab running simultaneously and they all need to play nicely together. Database operations Database migrations present a unique challenge in our Canary deployment model. We need schema changes to support new features while maintaining our ability to roll back if issues arise. Our solution involves careful separation of concerns: Regular migrations: Run during the Canary stage, designed to be backward-compatible, consists of only reversible changes Post-deploy migrations: The \"point of no return\" migrations that happen only after multiple successful deployments Database changes are handled with precision and extensive validation procedures: graph LR\n      A[Regular Migrations] --> B[Canary Stage Deploy]\n      B --> C[Main Stage Deploy]\n      C --> D[Post Deploy Migrations] Post-deploy migrations GitLab deployments involve many components. Updating GitLab is not atomic, so many components must be backward-compatible. Post-deploy migrations often contain changes that can't be easily rolled back — think data transformations, column drops, or structural changes that would break older code versions. By running them after we've gained confidence through multiple successful deployments, we ensure: The new code is stable and we're unlikely to need a rollback Performance characteristics are well understood in production Any edge cases have been discovered and addressed The blast radius is minimized if something does go wrong This approach provides the optimal balance: enabling rapid feature deployment through Canary releases while maintaining rollback capabilities until we have high confidence in deployment stability. The expand-migrate-contract pattern: Our database, frontend, and application compatibility changes follow a carefully orchestrated three-phase approach. Expand: Add new structures (columns, indexes) while keeping old ones functional Migrate: Deploy new application code that uses the new structures Contract: Remove old structures in post-deploy migrations after everything is stable Real-world example: When adding a new merge_readiness column to merge requests: Expand: Add the new column with a default value; existing code ignores it Migrate: Deploy code that reads and writes to the new column while still supporting the old approach 3 Contract: After several successful deployments, remove the old column in a post-deploy migration All database operations, application code, frontend code, and more, are subject to a set of guidelines that Engineering must adhere to, which can be found in our Multi-Version Compatibility documentation . Results and impact Our deployment infrastructure delivers measurable benefits: For GitLab Up to 12 deployments daily to GitLab.com Zero-downtime deployments serving millions of developers Security patches can reach production within hours, not days New features validated in production at massive scale before general availability For customers Proven deployment patterns you can adopt for your own applications Features battle-tested on the world's largest GitLab instance before reaching your environment Documentation that reflects actual production practices, not theoretical best practices Confidence that GitLab's recommended upgrade procedures work at any scale Key takeaways for engineering teams GitLab's deployment pipeline represents a sophisticated system that balances deployment velocity with operational reliability. The progressive deployment model, comprehensive testing integration, and robust rollback capabilities provide a foundation for reliable software delivery at scale. For engineering teams implementing similar systems, key considerations include: Automated testing: Comprehensive test coverage throughout the deployment pipeline Progressive rollout: Staged deployments to minimize risk and enable rapid recovery Monitoring integration: Comprehensive observability across all deployment stages Incident response: Rapid detection and resolution capabilities for deployment issues GitLab's architecture demonstrates how modern CI/CD systems can manage the complexity of large-scale deployments while maintaining the velocity required for competitive software development. Important note on scope This article specifically covers the deployment pipeline for services that are part of the GitLab Omnibus package and Helm chart — essentially the core GitLab monolith and its tightly integrated components. However, GitLab's infrastructure landscape extends beyond what's described here. Other services, notably our AI services and services that might be in a proof of concept state , follow a different deployment approach using our internal platform called Runway. If you're working with or curious about these other services, you can find more information in the Runway documentation . Other offerings, such as GitLab Dedicated are deployed more in alignment with what we expect customers to be capable of performing themselves by way of the GitLab Environment Toolkit . If you'd like to learn more, check out the GitLab Environment Toolkit project . The deployment strategies, architectural considerations, and pipeline complexities outlined in this article represent the battle-tested approach we use for our core platform — but like any large engineering organization, we have multiple deployment strategies tailored to different service types and maturity levels. Further documentation about Auto-Deploy and our procedures can be found at the below links: Engineering Deployments Release Procedural Documentation More resources How we decreased GitLab repo backup times from 48 hours to 41 minutes How we supercharged GitLab CI statuses with WebSockets How we reduced MR review time with Value Stream Management",
      "published_ts": 1764547200,
      "source_name": "GitLab Engineering",
      "content_type": "rex"
    }
  ],
  "hors_sujet": [],
  "lake_storage_formats": [
    {
      "url": "https://www.databricks.com/blog/expensive-delta-lake-s3-storage-mistakes-and-how-fix-them",
      "title": "Expensive Delta Lake S3 Storage Mistakes (And How to Fix Them)",
      "summary": "1. Introduction: The FoundationCloud object storage, such as S3, is the foundation...",
      "published_ts": 1764967200,
      "source_name": "Databricks Blog",
      "content_type": "technical"
    }
  ],
  "news": [
    {
      "url": "https://blog.cloudflare.com/5-december-2025-outage/",
      "title": "Cloudflare outage on December 5, 2025",
      "summary": "Cloudflare experienced a significant traffic outage on  December 5, 2025, starting approximately at 8:47 UTC. The incident lasted approximately 25 minutes before resolution. We are sorry for the impact that it caused to our customers and the Internet. The incident was not caused by an attack and was due to configuration changes being applied to attempt to mitigate a recent industry-wide vulnerability impacting React Server Components.",
      "published_ts": 1764892800,
      "source_name": "Cloudflare Engineering",
      "content_type": "technical"
    },
    {
      "url": "https://www.databricks.com/blog/bps-geospatial-ai-engine-transforming-safety-and-operations-databricks",
      "title": "BP’s Geospatial AI Engine: Transforming Safety and Operations with Databricks",
      "summary": "The integration of DATABRICKS capabilities with geospatial technology marks a significant...",
      "published_ts": 1764889500,
      "source_name": "Databricks Blog",
      "content_type": "technical"
    },
    {
      "url": "https://www.databricks.com/blog/mcp-powered-financial-ai-workflows-databricks",
      "title": "MCP-Powered Financial AI Workflows on Databricks",
      "summary": "To understand the foundations of Model Context Protocol (MCP) and Agent Bricks, see...",
      "published_ts": 1764889200,
      "source_name": "Databricks Blog",
      "content_type": "technical"
    },
    {
      "url": "https://stripe.com/blog/analyzing-how-saas-platforms-are-shipping-payments-and-finance-products-in-days",
      "title": "Analyzing how SaaS platforms are shipping payments and finance products in days",
      "summary": "Leading SaaS platforms across industries—from Squarespace to Jobber—are using embedded components to ship payments and finance features with minimal code. We analyzed the data to see which types of platforms are getting the most value.",
      "published_ts": 1764806400,
      "source_name": "Stripe Engineering Blog",
      "content_type": "technical"
    },
    {
      "url": "https://azure.microsoft.com/en-us/blog/a-decade-of-open-innovation-celebrating-10-years-of-microsoft-and-red-hat-partnership/",
      "title": "A decade of open innovation: Celebrating 10 years of Microsoft and Red Hat partnership",
      "summary": "A decade-long partnership between Microsoft and Red Hat has redefined what open innovation means, empowering customers with flexible, secure, and scalable solutions across the cloud. The post A decade of open innovation: Celebrating 10 years of Microsoft and Red Hat partnership appeared first on Microsoft Azure Blog .",
      "published_ts": 1764691200,
      "source_name": "Azure Blog",
      "content_type": "technical"
    },
    {
      "url": "https://azure.microsoft.com/en-us/blog/azure-networking-updates-on-security-reliability-and-high-availability/",
      "title": "Azure networking updates on security, reliability, and high availability",
      "summary": "The cloud landscape is evolving at an unprecedented pace, driven by the exponential growth of AI workloads. The post Azure networking updates on security, reliability, and high availability appeared first on Microsoft Azure Blog .",
      "published_ts": 1764608400,
      "source_name": "Azure Blog",
      "content_type": "technical"
    },
    {
      "url": "https://www.databricks.com/blog/databricks-neurips-2025",
      "title": "Databricks at NeurIPS 2025",
      "summary": "Databricks is proud to be a platinum sponsor of NeurIPS 2025. The conference runs...",
      "published_ts": 1764601200,
      "source_name": "Databricks Blog",
      "content_type": "technical"
    },
    {
      "url": "https://blog.cloudflare.com/why-replicate-joining-cloudflare/",
      "title": "Why Replicate is joining Cloudflare",
      "summary": "Today, we’re excited to announce that Replicate is officially part of Cloudflare. We wanted to share a bit about our journey and why we made this decision.",
      "published_ts": 1764568800,
      "source_name": "Cloudflare Engineering",
      "content_type": "technical"
    }
  ],
  "news_general": [],
  "python_analytics": [
    {
      "url": "https://www.uber.com/blog/improving-mysql-cluster-uptime-part2/",
      "title": "Improving MySQL® Cluster Uptime: Making MGR Viable at Scale",
      "summary": "Dive into the implementation, automation and failover logic that made MySQL® Group Replication viable at Uber scale.",
      "published_ts": 1764856800,
      "source_name": "Uber Engineering Blog",
      "content_type": "technical"
    }
  ],
  "warehouses_engines": [
    {
      "url": "https://www.databricks.com/blog/databricks-wins-seven-2025-aws-partner-year-awards",
      "title": "Databricks Wins Seven 2025 AWS Partner of the Year Awards",
      "summary": "We’re excited to announce that Databricks has received a record number of honors...",
      "published_ts": 1764897000,
      "source_name": "Databricks Blog",
      "content_type": "technical"
    },
    {
      "url": "https://www.databricks.com/blog/how-we-debug-1000s-databases-ai-databricks",
      "title": "How We Debug 1000s of Databases with AI at Databricks",
      "summary": "At Databricks, we’ve replaced manual database operations with AI, reducing time spent...",
      "published_ts": 1764799200,
      "source_name": "Databricks Blog",
      "content_type": "technical"
    }
  ]
}